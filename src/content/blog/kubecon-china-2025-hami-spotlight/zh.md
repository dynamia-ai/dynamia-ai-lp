---
title: "从顺丰科技到 AWS | 密瓜智能 HAMi 项目在 KubeCon China 广受行业巨头关注"
slug: "kubecon-china-2025-hami-spotlight"
date: "2025-06-26"
excerpt: "KubeCon China 2025 盛大落幕，密瓜智能 HAMi 项目作为 CNCF Sandbox 开源项目大放异彩，获得从顺丰科技到 AWS 等行业巨头的高度关注和认可。"
author: "密瓜智能团队"
tags: ["KubeCon", "HAMi", "GPU共享", "云原生", "Kubernetes", "AI基础设施"]
coverImage: "/images/blog/kubecon-china-2025/kubecon-china-2025.png"
language: "zh"
---

# 从顺丰科技到 AWS | 密瓜智能 HAMi 项目在 KubeCon China 广受行业巨头关注

KubeCon China 2025 在中国香港盛大落幕，这场云原生领域的顶级盛会汇聚了全球的技术专家、开发者与开源爱好者。在这股技术的浪潮中，由密瓜智能核心团队成员发起并主导的 HAMi 作为行业内唯一专注于异构 GPU 资源共享的 CNCF Sandbox 开源项目，不仅在技术议题上发出了响亮的声音，更在与全球社区的互动中展现了蓬勃的生命力。

本文将带您回顾密瓜智能团队与 HAMi 在本次 KubeCon 的高光时刻，分享台前幕后的精彩故事，并结合大会的整体趋势，为您深度剖析云原生环境下 GPU 技术生态的现在与未来。

## 高光时刻：HAMi 在 KubeCon China 上的两大核心议题

作为 GPU 共享与调度领域的创新力量，密瓜智能团队在本次大会上分享了两个核心议题，直面当前 Kubernetes 用户在 AI 时代面临的 GPU 资源管理痛点。

### 议题一：K8s issue #52757: 多容器共享 GPU

这个议题源于 Kubernetes 社区长达八年的难题：如何让多个容器高效共享一张昂贵的物理 GPU，同时提供隔离性。传统的 GPU 分配方式（如独占、Time-slicing、MPS、MIG 等）各有局限，在隔离性、灵活性、硬件兼容性和成本之间难以兼顾。

HAMi 的核心定位之一是提供一个开源、细粒度（支持算力百分比和显存 MB 级别）的灵活 GPU 共享解决方案，并且对应用程序无侵入、零改动。它通过调度器和设备插件协同工作，并在容器内部通过拦截 CUDA API 调用来实现算力和显存的预期控制，确保共享与隔离。此外，HAMi 还具备对国产 AI 芯片等异构硬件的广泛兼容性，能实现统一调度。

![尹玉演讲](/images/blog/kubecon-china-2025/speaker-yin-yu.png)
*尹玉（Dynamia 密瓜智能产品负责人，核心维护者 @HAMi）发表演讲*

### 议题二：智能 GPU管理：AI工作负载的动态池化、共享和调度

此议题深入探讨了在 K8s 集群中 GPU 管理面临的 "不可能三角" 困境——即难以同时满足高性能、强灵活性和高复用性，导致集群 GPU 利用率通常极低，因为 K8s 倾向于独占分配 GPU。现有的 NVIDIA 方案也往往配置复杂或缺乏灵活性。

HAMi 提出的解决方案旨在实现 GPU 的智能管理、动态池化、共享和调度。它极大地简化了用户体验，用户只需声明所需显存大小，无需关心底层硬件或具体的切分技术。核心在于通过 HAMi Core 拦截 CUDA API 调用，实现显存分配的硬隔离，并创新性地引入了动态 MIG 功能，与 Volcano 调度器和 HAMi 设备插件深度结合，可以根据任务需求动态生成 MIG 切片，解决了 MIG 预配置的痛点，同时兼顾了性能与隔离性。这套方案显著提高了 GPU 利用率，并在很大程度上解决了 "不可能三角" 问题。

![技术团队演讲](/images/blog/kubecon-china-2025/team-presentation.png)
*（从左至右）李孟轩（Dynamia 密瓜智能 Co-Founder & CTO & HAMi 发起人）、陈薇（中国联通云数据中心技术专家）*

## 台前幕后：社区的力量与思想的碰撞

除了精彩的议题分享，密瓜智能团队和 HAMi 在 KubeCon 现场也收获了满满的认可与宝贵的建议。这些瞬间，是技术之外，社区最真实的温度。

### 故事一：从听众到同行

在早上的闪电演讲（Lightning Talk）结束后，几位听众朋友立刻找到我们，他们不仅对 HAMi 表达了浓厚的兴趣，更关切地询问下午深度演讲的具体时间和地点，期待着更深入的交流。这种即时的反馈，是对项目价值最直接的肯定！

### 故事二：与 CNCF CTO 的深度对话

![与 CNCF CTO 合影](/images/blog/kubecon-china-2025/cncf-cto-meeting.png)
*（从左至右） Keith Chan (Linux Foundation APAC, CNCF China Director)、Chris Aniszczyk (CNCF CTO)、李孟轩、尹玉、陈文*

密瓜智能的 HAMi Maintainer 团队与 CNCF CTO Chris Aniszczyk 进行了深入的交流。Chris 对 HAMi 项目表达了高度的认可与支持，并从社区发展的战略高度，给出了极具价值的建议：

- **构建系统性用户案例**：强调了详实、成体系的用户案例对于提升项目可信度和传播力的关键作用。

- **探索全球开发者参与**：推荐我们积极利用 CNCF 的 LFX Mentorship 项目，这是一个吸引全球高校和个人开发者的绝佳途径。

- **积极参与 CNCF 全球活动**：鼓励我们更多参与 KCD (Kubernetes Community Days) 以及在韩国、日本、印度等区域的线上线下活动。Chris 也特别提到了我们在 Slack 中分享的越南电信用户案例，认为这是一个很好的开端。这启发我们，未来需要更主动地与这些区域的开发者建立联系，鼓励他们带着自己的实践故事走上分享的舞台。

- **建立专业的安全响应机制**：强调了当项目从 Sandbox 迈向更高成熟度时，建立规范的漏洞披露与安全响应流程是构筑社区信任的基石。

- **明确孵化路线图**：帮助我们进一步梳理了进入 CNCF 孵化阶段需要关注的关键时间节点和准备事项。

### 故事三：来自官方 Keynote 的高光认可

第二天的大会 Keynote 上，出现了令人振奋的一幕。Linux 基金会亚太区、CNCF 中国区总监 Keith Chan 在他的开幕致辞中，用单独一页幻灯片单独介绍了我们的 HAMi 项目！引用了顺丰科技近期发布的《Effective GPU 技术白皮书》，展示了 HAMi 在 GPU 池化与调度方面的工程实践。这无疑是来自官方的极高规格认可。

![Keynote 展示](/images/blog/kubecon-china-2025/keynote-hami.png)
*Keith Chan 在 Keynote 中展示 HAMi 项目*

### 故事四：独立展台的思想碰撞

在开源项目的独立展台区，HAMi 同样吸引了众多关注者。其中，来自 AWS 的高级容器解决方案架构师 Frank Fan 与我们进行了长时间的深度交流。他详细询问了 HAMi 的虚拟化实现原理，并将其与其他技术方案进行了细致的对比探讨。并且在最后，我们团队的 HAMi 作者李孟轩被当场邀请参加第二天的 AWS Pioneer Forum，分享云原生与 AI 创新的更多可能。

![AWS 交流](/images/blog/kubecon-china-2025/aws-discussion.png)

### 故事五：午餐桌上的"手动点赞"

缘分总是如此奇妙。中午用餐时，我们巧遇了来自顺丰科技 AI 团队的朋友。他们对 HAMi 在其实际生产环境中的应用效果表示了高度赞扬，并当面为项目"手动点赞"。这种来自一线用户的真实反馈，是我们持续前行的最大动力。

## 洞察 KubeCon：GPU 技术生态的现在与未来

纵观本次 KubeCon，关于 GPU 的议题热度空前。在 AI 与大模型（LLM）席卷全球的背景下，如何更高效、经济、灵活地利用这些昂贵的计算资源，已成为整个行业共同的焦点。密瓜智能 HAMi 的探索并非孤例，而是整个技术浪潮中的一朵关键浪花。

### 核心挑战：成本与效率的"不可能三角"

![GPU 利用率挑战](/images/blog/kubecon-china-2025/gpu-utilization-challenge.png)

所有讨论几乎都围绕着几个核心痛点展开：

- **高成本与低利用率**：GPU 硬件昂贵，但在实际生产中，利用率低下和资源碎片化造成的浪费触目惊心，严重影响了投资回报率。

- **LLM 工作负载的特殊性**：大模型的输入输出长度不固定，导致传统的 QPS、并发量等指标失真，使得资源评估和弹性伸缩变得异常困难。DCGM 等工具暴露的底层利用率指标也难以真实反映 GPU 的"空间占有率"。

- **调度与扩容的延迟**：从指标采集到容器启动、模型加载，整个链路的延迟高达数分钟，往往导致资源就绪时流量高峰已过，系统响应滞后。

### 技术趋势：走向精细化、自动化与全栈优化

![技术发展趋势](/images/blog/kubecon-china-2025/tech-trends.png)

面对挑战，社区正在从多个层面寻求突破，形成了一套组合拳：

**Kubernetes 成为 AI基础设施的绝对核心**：无论是利用 Kubespray 在裸金属上构建高性能集群，还是通过 Operator 实现自动化运维（如 Vivo 内部的 KubeOps 实践），K8s 都提供了统一、可移植、高可用的底座。

**GPU资源管理的精细化与虚拟化**：

- *硬件层*：MIG 和 MPS 作为 NVIDIA 提供的基础切分技术，分别适用于训练和推理场景。

- *软件层*：API 拦截技术成为主流。HAMi 所采用的 CUDA API 拦截，实现了对算力和显存的硬性隔离与控制。同时，大会也出现了新的思路，如将拦截层面提升到 Python 级别（在 PyTorch/TensorFlow 框架层拦截），这种方式更灵活且理论上硬件无关，为解决"GPU 垄断"和异构支持提供了新方向。

**调度与编排的智能化**：

- *批处理与组调度*：以 Volcano 为代表的批处理调度器通过 Gang Scheduling 机制，确保关联任务"同生共死"，有效解决了资源碎片化，大幅缩短排队时间。

- *LLM 专用编排*：针对 LLM 推理的 P/D 分离（Prefill/Decode）架构，需要更复杂的角色组编排（Role Group Orchestration）来协同不同角色的 Pod，并感知拓扑结构以优化性能。

**LLM推理性能的全栈优化**：

AIBrix 项目展示了全栈优化的威力，通过 KVCache Offloading 将 KV Cache 卸载到更低成本的存储（如 DRAM）来降低对 GPU 高带宽内存（HBM）的占用，并以此优化首字延迟 (TTFT)，通过 P/D 分离降低后续生成延迟（TPOT），尤其在处理长输出时能提升性能稳定性。并通过 AIGateway 实现基于令牌和缓存感知的智能路由。LoRA 的高密度部署方案也为管理海量微调模型提供了降本增效的利器，尤其适用于长尾模型。

### 未来展望：融合、简化与开放

![未来发展方向](/images/blog/kubecon-china-2025/future-outlook.png)

- **"少即是多"**：像 Kubespray 这样的基础工具正在精简功能，聚焦于生成标准化的 K8s 集群，降低复杂性和维护成本。

- **深度云原生整合**：AI 基础设施将与 Envoy、vLLM 等云原生组件深度融合，形成从流量入口到计算核心的全栈解决方案。

- **拥抱硬件多样性**：对非 NVIDIA GPU 的统一支持将是未来的关键，抽象层和灵活的虚拟化技术将扮演重要角色。

- **从被动到主动**：基于预测的数据驱动扩缩容将取代被动响应，以应对 LLM 负载的突发性。

## 结语

KubeCon China 2025 是一次思想的盛宴，也是密瓜智能核心团队和 HAMi 社区成长道路上的一个重要里程碑。

通过与全球开发者的深入交流和对整体技术趋势的洞察，我们愈发笃定一个对未来的判断：在 AI Infra 领域，单纯优化单一环节或组件的时代正在过去，真正的决胜之道在于全栈优化。

HAMi 目前在 GPU 共享与调度这一核心层面积累了深厚的经验，但这仅仅是开始。我们坚信，想要在 AI Infra 的长征中走得更远，就必须打破壁垒，拥抱更广阔的生态。

未来，我们将主动与生态上下游的优秀项目探索更多合作的可能，与整个云原生社区一同为用户提供从资源到底层、从调度到应用无缝衔接的端到端解决方案。这不仅是 HAMi 的未来之路，也是密瓜智能团队对所有社区伙伴的诚挚邀请。

---

*想了解更多 HAMi 项目信息，请访问 [GitHub 仓库](https://github.com/Project-HAMi/HAMi) 或加入我们的 [Slack 社区](https://cloud-native.slack.com/archives/C07T10BU4R2)。* 